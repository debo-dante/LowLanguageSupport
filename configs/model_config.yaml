# Model Configuration for Indian Language Models
# This file contains default configurations for training and fine-tuning

# Base model configuration
model:
  type: "bert"  # bert, roberta, gpt2, custom
  vocab_size: 50000
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  max_position_embeddings: 1024  # Extended for longer Indian texts
  dropout_prob: 0.1
  layer_norm_eps: 1e-12
  
# Language-specific settings
language:
  primary: "hi"  # Primary language
  supported: ["hi", "bn", "ta", "te", "mr", "gu", "pa", "or", "ml", "kn"]
  script_embeddings: true  # Enable script-specific embeddings
  multilingual: true       # Enable multilingual support
  
# Training configuration
training:
  batch_size: 16
  learning_rate: 2e-5
  num_epochs: 10
  warmup_steps: 1000
  weight_decay: 0.01
  max_grad_norm: 1.0
  gradient_accumulation_steps: 4
  eval_steps: 500
  save_steps: 1000
  logging_steps: 100
  
  # Optimizer settings
  optimizer: "adamw"
  adam_epsilon: 1e-8
  beta1: 0.9
  beta2: 0.999
  
  # Scheduler settings
  scheduler: "linear_with_warmup"
  
# Data configuration
data:
  max_sequence_length: 512
  pad_to_max_length: false
  preprocessing:
    normalize_unicode: true
    remove_urls: true
    remove_emails: true
    remove_phone_numbers: true
    normalize_digits: true
    remove_extra_whitespace: true
    
# Tokenizer configuration
tokenizer:
  model_type: "wordpiece"  # wordpiece, sentencepiece, bpe
  vocab_size: 50000
  min_frequency: 2
  special_tokens:
    - "[PAD]"
    - "[UNK]"
    - "[CLS]"
    - "[SEP]"
    - "[MASK]"
  
# Adapter configuration (for efficient fine-tuning)
adapters:
  enabled: true
  reduction_factor: 4  # hidden_size // reduction_factor
  dropout: 0.1
  
# Evaluation configuration
evaluation:
  tasks: ["language_modeling", "classification", "similarity", "cross_lingual"]
  metrics: ["accuracy", "f1", "perplexity", "bleu"]
  languages: ["hi", "bn", "ta", "te"]
  
# Hardware configuration
hardware:
  device: "auto"  # auto, cuda, cpu
  mixed_precision: true
  dataloader_num_workers: 4
  pin_memory: true
  
# Logging and monitoring
logging:
  level: "INFO"
  use_wandb: false
  project_name: "indian-language-nlp"
  experiment_name: null  # Will be auto-generated if null
  
# Model saving
saving:
  output_dir: "./data/models"
  save_best_model: true
  save_total_limit: 3
  load_best_model_at_end: true
