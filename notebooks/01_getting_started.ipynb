{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Indian Language NLP\n",
    "\n",
    "This notebook demonstrates how to use the Indian Language NLP framework to:\n",
    "1. Collect data for underrepresented Indian languages\n",
    "2. Preprocess and clean the text data\n",
    "3. Train language models optimized for Indian languages\n",
    "4. Evaluate model performance across various tasks\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have installed all dependencies:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our custom modules\n",
    "from data_collection import WebScraper\n",
    "from preprocessing import TextCleaner\n",
    "from models import IndianLanguageModel\n",
    "from evaluation import ModelEvaluator\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "Let's start by collecting some Hindi text data from news websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize web scraper for Hindi\n",
    "scraper = WebScraper(\n",
    "    language='hi',\n",
    "    delay=2.0,  # Be respectful to websites\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "# Define some Hindi news sites\n",
    "hindi_news_sites = [\n",
    "    {'name': 'BBC Hindi', 'url': 'https://www.bbc.com/hindi'},\n",
    "    {'name': 'NDTV India', 'url': 'https://khabar.ndtv.com/'}\n",
    "]\n",
    "\n",
    "print(f\"üîç Starting data collection for Hindi...\")\n",
    "\n",
    "# Note: This is a demo - be careful with actual scraping\n",
    "# Make sure to respect robots.txt and terms of service\n",
    "sample_articles = []\n",
    "\n",
    "# For demonstration, we'll create some sample data instead of actual scraping\n",
    "sample_data = {\n",
    "    'url': 'https://example.com/hindi-article',\n",
    "    'title': '‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§®‡§à ‡§§‡§ï‡§®‡•Ä‡§ï ‡§ï‡§æ ‡§µ‡§ø‡§ï‡§æ‡§∏',\n",
    "    'content': '‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§§‡§ï‡§®‡•Ä‡§ï‡•Ä ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§§‡•á‡§ú‡•Ä ‡§∏‡•á ‡§¨‡§¢‡§º ‡§∞‡§π‡§æ ‡§π‡•à‡•§ ‡§®‡§à ‡§ñ‡•ã‡§ú‡•á‡§Ç ‡§î‡§∞ ‡§®‡§µ‡§æ‡§ö‡§æ‡§∞ ‡§¶‡•á‡§∂ ‡§ï‡•ã ‡§Ü‡§ó‡•á ‡§¨‡§¢‡§º‡§æ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§',\n",
    "    'language': 'hi',\n",
    "    'source': 'Demo'\n",
    "}\n",
    "\n",
    "sample_articles = [sample_data] * 10  # Create 10 sample articles\n",
    "\n",
    "print(f\"üì∞ Collected {len(sample_articles)} sample articles\")\n",
    "print(f\"Sample article title: {sample_articles[0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "Now let's clean and preprocess our Hindi text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text cleaner for Hindi\n",
    "cleaner = TextCleaner(language='hi')\n",
    "\n",
    "print(\"üßπ Cleaning text data...\")\n",
    "\n",
    "# Clean the sample text\n",
    "sample_text = sample_articles[0]['content']\n",
    "print(f\"Original text: {sample_text}\")\n",
    "\n",
    "# Apply various cleaning operations\n",
    "cleaned_text = cleaner.clean_text(\n",
    "    sample_text,\n",
    "    normalize_unicode=True,\n",
    "    remove_extra_whitespace=True,\n",
    "    normalize_digits=True\n",
    ")\n",
    "\n",
    "print(f\"Cleaned text: {cleaned_text}\")\n",
    "\n",
    "# Get text statistics\n",
    "stats = cleaner.get_text_statistics(sample_text)\n",
    "print(\"\\nüìä Text Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = cleaner.sentence_tokenize(sample_text)\n",
    "print(f\"\\nüìù Found {len(sentences)} sentences:\")\n",
    "for i, sentence in enumerate(sentences[:3]):  # Show first 3\n",
    "    print(f\"  {i+1}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Let's create and initialize an Indian language model optimized for Hindi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Indian Language Model for Hindi\n",
    "print(\"ü§ñ Initializing Indian Language Model...\")\n",
    "\n",
    "model = IndianLanguageModel(\n",
    "    language='hi',\n",
    "    model_type='bert',\n",
    "    vocab_size=30000,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=6,  # Smaller model for demo\n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=512\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model initialized successfully!\")\n",
    "\n",
    "# Get model parameter information\n",
    "param_count = model.get_parameter_count()\n",
    "print(\"\\nüìà Model Parameters:\")\n",
    "for component, count in param_count.items():\n",
    "    print(f\"  {component}: {count:,} parameters\")\n",
    "\n",
    "print(f\"\\nüîß Total parameters: {param_count['total']:,}\")\n",
    "\n",
    "# Test the model with some sample text\n",
    "sample_texts = [\n",
    "    \"‡§≠‡§æ‡§∞‡§§ ‡§è‡§ï ‡§Æ‡§π‡§æ‡§® ‡§¶‡•á‡§∂ ‡§π‡•à‡•§\",\n",
    "    \"‡§§‡§ï‡§®‡•Ä‡§ï ‡§ï‡§æ ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§§‡•á‡§ú‡•Ä ‡§∏‡•á ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à‡•§\",\n",
    "    \"‡§π‡§Æ‡•á‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§≠‡§æ‡§∑‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§∏‡§Ç‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß† Testing model embeddings...\")\n",
    "embeddings = model.get_embeddings(sample_texts, language='hi')\n",
    "print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Sample embedding (first 5 dimensions): {embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "Let's evaluate our model using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model evaluator\n",
    "evaluator = ModelEvaluator(\n",
    "    model=model,\n",
    "    language='hi',\n",
    "    device='cpu'  # Use CPU for demo\n",
    ")\n",
    "\n",
    "print(\"üìä Starting model evaluation...\")\n",
    "\n",
    "# Create some sample evaluation data\n",
    "eval_texts = [\n",
    "    \"‡§Ø‡§π ‡§è‡§ï ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§π‡•à‡•§\",\n",
    "    \"‡§Æ‡•Å‡§ù‡•á ‡§Ø‡§π ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§™‡§∏‡§Ç‡§¶ ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§à‡•§\",\n",
    "    \"‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à‡•§\",\n",
    "    \"‡§Æ‡•à‡§Ç ‡§ñ‡•Å‡§∂ ‡§π‡•Ç‡§Ç‡•§\",\n",
    "    \"‡§Ø‡§π ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞ ‡§¶‡•Å‡§ñ‡§¶ ‡§π‡•à‡•§\"\n",
    "]\n",
    "\n",
    "# Sample labels for sentiment (0=negative, 1=neutral, 2=positive)\n",
    "eval_labels = [2, 0, 2, 2, 0]\n",
    "\n",
    "# Evaluate text classification\n",
    "classification_results = evaluator.evaluate_text_classification(\n",
    "    texts=eval_texts,\n",
    "    labels=eval_labels,\n",
    "    task_name=\"sentiment_analysis\"\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Classification Results:\")\n",
    "for metric, value in classification_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    elif metric != 'classification_report':\n",
    "        print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Test semantic similarity\n",
    "text_pairs = [\n",
    "    (\"‡§Æ‡•Å‡§ù‡•á ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•à\", \"‡§Æ‡•à‡§Ç ‡§ñ‡•Å‡§∂ ‡§π‡•Ç‡§Ç\"),\n",
    "    (\"‡§Ø‡§π ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\", \"‡§Ø‡§π ‡§¨‡•Å‡§∞‡§æ ‡§π‡•à\"),\n",
    "    (\"‡§Ü‡§ú ‡§¨‡§æ‡§∞‡§ø‡§∂ ‡§π‡•à\", \"‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\")\n",
    "]\n",
    "\n",
    "similarity_scores = [0.9, 0.1, 0.5]  # Ground truth similarities\n",
    "\n",
    "similarity_results = evaluator.evaluate_semantic_similarity(\n",
    "    text_pairs=text_pairs,\n",
    "    similarity_scores=similarity_scores\n",
    ")\n",
    "\n",
    "print(\"\\nüîÑ Semantic Similarity Results:\")\n",
    "for metric, value in similarity_results.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-lingual Evaluation\n",
    "\n",
    "Let's test the model's ability to handle multiple Indian languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multilingual test data\n",
    "multilingual_data = {\n",
    "    'hi': {\n",
    "        'texts': [\n",
    "            \"‡§Ø‡§π ‡§è‡§ï ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§¶‡§ø‡§® ‡§π‡•à‡•§\",\n",
    "            \"‡§Æ‡•Å‡§ù‡•á ‡§Ø‡§π ‡§™‡§∏‡§Ç‡§¶ ‡§π‡•à‡•§\",\n",
    "            \"‡§§‡§ï‡§®‡•Ä‡§ï ‡§¨‡§π‡•Å‡§§ ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§π‡•à‡•§\"\n",
    "        ],\n",
    "        'labels': [1, 1, 1]  # Positive sentiment\n",
    "    },\n",
    "    'bn': {\n",
    "        'texts': [\n",
    "            \"‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡¶æ‡¶≤ ‡¶¶‡¶ø‡¶®‡•§\",  # This is a good day\n",
    "            \"‡¶Ü‡¶Æ‡¶ø ‡¶è‡¶ü‡¶æ ‡¶™‡¶õ‡¶®‡ßç‡¶¶ ‡¶ï‡¶∞‡¶ø‡•§\",  # I like this\n",
    "            \"‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶ñ‡ßÅ‡¶¨ ‡¶â‡¶™‡¶ï‡¶æ‡¶∞‡ßÄ‡•§\"  # Technology is very useful\n",
    "        ],\n",
    "        'labels': [1, 1, 1]  # Positive sentiment\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üåç Evaluating multilingual capabilities...\")\n",
    "\n",
    "# Evaluate multilingual performance\n",
    "multilingual_results = evaluator.evaluate_multilingual_capabilities(\n",
    "    multilingual_data\n",
    ")\n",
    "\n",
    "print(\"\\nüó∫Ô∏è Multilingual Results:\")\n",
    "print(f\"  Supported languages: {multilingual_results['supported_languages']}\")\n",
    "print(f\"  Consistency score: {multilingual_results['consistency_score']:.4f}\")\n",
    "\n",
    "# Language-specific results\n",
    "for lang, results in multilingual_results['language_results'].items():\n",
    "    print(f\"\\n  {lang.upper()} Language Results:\")\n",
    "    for metric, value in results.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"    {metric}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"    {metric}: {value}\")\n",
    "\n",
    "# Cross-lingual transfer evaluation\n",
    "if len(multilingual_data) >= 2:\n",
    "    source_lang = 'hi'\n",
    "    target_lang = 'bn'\n",
    "    \n",
    "    cross_lingual_results = evaluator.evaluate_cross_lingual_transfer(\n",
    "        source_data=multilingual_data[source_lang],\n",
    "        target_data=multilingual_data[target_lang],\n",
    "        source_lang=source_lang,\n",
    "        target_lang=target_lang\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüîÑ Cross-lingual Transfer ({source_lang} ‚Üí {target_lang}):\")\n",
    "    for metric, value in cross_lingual_results.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Comprehensive Report\n",
    "\n",
    "Let's generate a comprehensive evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation report\n",
    "print(\"üìã Generating comprehensive evaluation report...\")\n",
    "\n",
    "report = evaluator.generate_evaluation_report(\n",
    "    output_path='../data/evaluation_report'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION REPORT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(report[:1000] + \"...\" if len(report) > 1000 else report)\n",
    "\n",
    "print(\"\\n‚úÖ Full report saved to: ../data/evaluation_report.json and .txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Create visualizations of the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "print(\"üìä Creating evaluation visualizations...\")\n",
    "\n",
    "try:\n",
    "    evaluator.visualize_results(\n",
    "        save_path='../data/evaluation_charts.png'\n",
    "    )\n",
    "    print(\"‚úÖ Visualizations saved to: ../data/evaluation_charts.png\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Visualization failed: {e}\")\n",
    "    print(\"This is normal in some environments - check the saved report files instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model\n",
    "\n",
    "Finally, let's save our trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = '../data/models/hindi_language_model'\n",
    "\n",
    "print(f\"üíæ Saving model to: {model_save_path}\")\n",
    "\n",
    "try:\n",
    "    model.save_model(model_save_path)\n",
    "    print(\"‚úÖ Model saved successfully!\")\n",
    "    \n",
    "    # Test loading the model\n",
    "    print(\"üîÑ Testing model loading...\")\n",
    "    loaded_model = IndianLanguageModel.load_model(model_save_path)\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "    # Verify the loaded model works\n",
    "    test_text = \"‡§Ø‡§π ‡§è‡§ï ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§π‡•à‡•§\"  # This is a test\n",
    "    embeddings = loaded_model.get_embeddings([test_text], language='hi')\n",
    "    print(f\"‚úÖ Loaded model working! Embedding shape: {embeddings.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving/loading model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "üéâ **Congratulations!** You've successfully completed the Indian Language NLP workflow:\n",
    "\n",
    "1. ‚úÖ **Data Collection**: Learned how to collect text data from web sources\n",
    "2. ‚úÖ **Text Preprocessing**: Cleaned and normalized Indian language text\n",
    "3. ‚úÖ **Model Training**: Initialized and configured an Indian language model\n",
    "4. ‚úÖ **Evaluation**: Assessed model performance across various tasks\n",
    "5. ‚úÖ **Cross-lingual**: Tested multilingual capabilities\n",
    "6. ‚úÖ **Reporting**: Generated comprehensive evaluation reports\n",
    "7. ‚úÖ **Visualization**: Created performance charts\n",
    "8. ‚úÖ **Model Saving**: Saved the model for future use\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore other notebooks in this repository for advanced topics\n",
    "- Try training on larger datasets\n",
    "- Experiment with different model architectures\n",
    "- Test with more Indian languages\n",
    "- Deploy your model for real-world applications\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [IndicNLP Library](https://github.com/anoopkunchukuttan/indic_nlp_library)\n",
    "- [AI4Bharat](https://ai4bharat.org/)\n",
    "- [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
    "- [Indian Language Datasets](http://www.cfilt.iitb.ac.in/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy coding and building bridges between technology and India's linguistic diversity!** üáÆüá≥‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
